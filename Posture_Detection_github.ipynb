{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Posture Detection github",
      "provenance": [],
      "collapsed_sections": [
        "O5wRn2CXMlk6"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAGt6TneLhLD"
      },
      "source": [
        "# Posture Detection with Baseline Model ( SVM, RF, NN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktUTb3G-Ltii"
      },
      "source": [
        "## import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-ma1on_01w6",
        "outputId": "63245f04-6638-4a89-b66e-d0639c82404a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBWAy--G3wji"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint \n",
        "#---Preprocess-------\n",
        "data_path = r\"/content/drive/MyDrive/Data_chair/position_arakawa_lab_labeled.csv\"\n",
        "\n",
        "#read data in \n",
        "raw_data= pd.read_csv(data_path) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E268OM8z274E"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization Function "
      ],
      "metadata": {
        "id": "tVEwsr2tufPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def norm_funct(d_frame):\n",
        "  columns_key = d_frame.columns.values\n",
        "  df_mean =  d_frame[columns_key].mean()\n",
        "  df_std =  d_frame[columns_key].std().replace(0,1)\n",
        "  df_norm = (d_frame[columns_key]-df_mean)/df_std\n",
        "  return df_norm \n",
        "\n",
        "# test\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "d1 = {'data1':np.zeros([5]),'data2':np.ones([5]),'data3':np.array([i for i in range(5)]),'data4':np.array([i for i in range(5,10)])}\n",
        "df_raw = pd.DataFrame(d1)\n",
        "df_1 = df_raw.copy()\n",
        "df_1 = norm_funct(df_1)\n"
      ],
      "metadata": {
        "id": "2qf8sNzTTK2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_raw.to_markdown())\n",
        "print(df_1)\n"
      ],
      "metadata": {
        "id": "7x1oI1Oyuvp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0qGz2pMa2ZN"
      },
      "source": [
        "Create the new 56 features.\n",
        "\n",
        "These features are the difference between each sensors' pressure values.\n",
        "By the theory of combination, selecting 2 sensors from 8 of them on backrest and seat pad gives us 2 x4 x 7 = 56 "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Method for creating new features\n",
        "\n",
        "def create_feature(data_set):\n",
        "    # seperate hips sensors from back senors\n",
        "    hip_keys= data_set.columns.values[:8]\n",
        "    #back_keys= data_set.columns.values[8:-2]\n",
        "    back_keys= data_set.columns.values[8:] # this line is for adding normalization\n",
        "\n",
        "    # calculation on hip data\n",
        "    sets_keys = hip_keys\n",
        "    for j in range(len(sets_keys)):\n",
        "      for i in range(j+1,len(sets_keys)) :\n",
        "        data_set[sets_keys[j]+'-'+sets_keys[i]]= data_set[sets_keys[j]]- data_set[sets_keys[i]]\n",
        "    \n",
        "    sets_keys = back_keys\n",
        "    for j in range(len(sets_keys)):\n",
        "      for i in range(j+1,len(sets_keys)) :\n",
        "        data_set[sets_keys[j]+'-'+sets_keys[i]]= data_set[sets_keys[j]]- data_set[sets_keys[i]]\n",
        "    \n",
        "    \n",
        "    print(f'The total number now is :{len(data_set.columns.values[:])}')   \n",
        "    print(f'The keys created now are :{data_set.columns.values[:]}')     \n",
        "    return data_set  \n",
        "  \n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "fJg83Y1aT29I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test code\n",
        "mod_data = raw_data.copy()\n",
        "mod_data = create_feature(mod_data)"
      ],
      "metadata": {
        "id": "9o4BL9qWPJK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the new 120 features.\n",
        "\n",
        "These features are the difference between each sensors' pressure values."
      ],
      "metadata": {
        "id": "UV5WkHknXGXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_feature_120(data_set):\n",
        "    #sets_keys = data_set.columns.values[:-2]\n",
        "    sets_keys = data_set.columns.values[:] # for adding normalization\n",
        "\n",
        "    for j in range(len(sets_keys)):\n",
        "      for i in range(j+1,len(sets_keys)) :\n",
        "        data_set[sets_keys[j]+'-'+sets_keys[i]]= data_set[sets_keys[j]]- data_set[sets_keys[i]]\n",
        "    print(f'The total number now is :{len(data_set.columns.values[:])}')    \n",
        "    print(f'The keys created now are :{data_set.columns.values[:]}')     \n",
        "     \n",
        "    return data_set  \n",
        "  "
      ],
      "metadata": {
        "id": "VgylHwKlXb5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test code\n",
        "mod_data = raw_data.copy()\n",
        "mod_data = create_feature_120(mod_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03867d0f-c60d-490f-ea9d-17dfab687f43",
        "id": "e2kjt6LqXxjU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total number now is :138\n",
            "The keys created now are :['hip1' 'hip2' 'hip3' 'hip4' 'hip5' 'hip6' 'hip7' 'hip8' 'back1' 'back2'\n",
            " 'back3' 'back4' 'back5' 'back6' 'back7' 'back8' 'label' 'personID'\n",
            " 'hip1-hip2' 'hip1-hip3' 'hip1-hip4' 'hip1-hip5' 'hip1-hip6' 'hip1-hip7'\n",
            " 'hip1-hip8' 'hip1-back1' 'hip1-back2' 'hip1-back3' 'hip1-back4'\n",
            " 'hip1-back5' 'hip1-back6' 'hip1-back7' 'hip1-back8' 'hip2-hip3'\n",
            " 'hip2-hip4' 'hip2-hip5' 'hip2-hip6' 'hip2-hip7' 'hip2-hip8' 'hip2-back1'\n",
            " 'hip2-back2' 'hip2-back3' 'hip2-back4' 'hip2-back5' 'hip2-back6'\n",
            " 'hip2-back7' 'hip2-back8' 'hip3-hip4' 'hip3-hip5' 'hip3-hip6' 'hip3-hip7'\n",
            " 'hip3-hip8' 'hip3-back1' 'hip3-back2' 'hip3-back3' 'hip3-back4'\n",
            " 'hip3-back5' 'hip3-back6' 'hip3-back7' 'hip3-back8' 'hip4-hip5'\n",
            " 'hip4-hip6' 'hip4-hip7' 'hip4-hip8' 'hip4-back1' 'hip4-back2'\n",
            " 'hip4-back3' 'hip4-back4' 'hip4-back5' 'hip4-back6' 'hip4-back7'\n",
            " 'hip4-back8' 'hip5-hip6' 'hip5-hip7' 'hip5-hip8' 'hip5-back1'\n",
            " 'hip5-back2' 'hip5-back3' 'hip5-back4' 'hip5-back5' 'hip5-back6'\n",
            " 'hip5-back7' 'hip5-back8' 'hip6-hip7' 'hip6-hip8' 'hip6-back1'\n",
            " 'hip6-back2' 'hip6-back3' 'hip6-back4' 'hip6-back5' 'hip6-back6'\n",
            " 'hip6-back7' 'hip6-back8' 'hip7-hip8' 'hip7-back1' 'hip7-back2'\n",
            " 'hip7-back3' 'hip7-back4' 'hip7-back5' 'hip7-back6' 'hip7-back7'\n",
            " 'hip7-back8' 'hip8-back1' 'hip8-back2' 'hip8-back3' 'hip8-back4'\n",
            " 'hip8-back5' 'hip8-back6' 'hip8-back7' 'hip8-back8' 'back1-back2'\n",
            " 'back1-back3' 'back1-back4' 'back1-back5' 'back1-back6' 'back1-back7'\n",
            " 'back1-back8' 'back2-back3' 'back2-back4' 'back2-back5' 'back2-back6'\n",
            " 'back2-back7' 'back2-back8' 'back3-back4' 'back3-back5' 'back3-back6'\n",
            " 'back3-back7' 'back3-back8' 'back4-back5' 'back4-back6' 'back4-back7'\n",
            " 'back4-back8' 'back5-back6' 'back5-back7' 'back5-back8' 'back6-back7'\n",
            " 'back6-back8' 'back7-back8']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data"
      ],
      "metadata": {
        "id": "C7DJM-RaXvFz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0cd934T9eNG"
      },
      "source": [
        "def preprocess_data(test_id,raw_set,make_feature='raw',norm =False):\n",
        "  # split data to train and test group based on PersonIDo\n",
        "  print(f'Spliting data {test_id} from raw_data.... ')\n",
        "\n",
        "  mod_data = raw_set.copy()\n",
        "  # try to drop some feature which seems to be unstable  \n",
        "  grouped_id = mod_data.groupby(\"personID\")\n",
        "\n",
        "  # list to store personid for training in this round\n",
        "  train_id = [i for i in range(1,11)]\n",
        "  train_id.remove(test_id)\n",
        "  train_df = []\n",
        "  test_df = [] \n",
        "  print(f'training set this round includes {train_id}  ')\n",
        "  \n",
        "  # concat the rest of data as train data\n",
        "  for i in train_id: \n",
        "    train_df.append(grouped_id.get_group(i)) \n",
        "\n",
        "  test_df.append(grouped_id.get_group(test_id)) \n",
        "  # back to dataframe\n",
        "  train_df= pd.concat(train_df)\n",
        "  test_df= pd.concat(test_df)\n",
        "\n",
        "\n",
        "\n",
        "  #Shuffle and concat train data\n",
        "  train_f=train_df.sample(frac=1).reset_index(drop=True)\n",
        "  #Shuffle test data\n",
        "  test_f=test_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "  # seperate label from raw data => label process finished\n",
        "  train_y = train_f[[\"label\"]].to_numpy().reshape([-1]) \n",
        "  test_y = test_f[[\"label\"]].to_numpy().reshape([-1])\n",
        "  \n",
        "  #-----Training data preprocess------  \n",
        "  train_x = train_f.drop(columns=[\"label\",\"personID\"])\n",
        "  test_x = test_f.drop(columns=[\"label\",\"personID\"])\n",
        "\n",
        "  #normalize if norm = True\n",
        "  if (norm):\n",
        "    train_x=norm_funct(train_x)\n",
        "    test_x=norm_funct(test_x)\n",
        "\n",
        "  if(make_feature=='fifty'):\n",
        "    train_x = create_feature(train_x)\n",
        "    test_x = create_feature(test_x)\n",
        "  elif(make_feature=='hundred'):\n",
        "    train_x = create_feature_120(train_x)\n",
        "    test_x = create_feature_120(test_x)\n",
        " \n",
        " # Transfer training data to numpy\n",
        "  train_x = train_x.to_numpy()\n",
        "  test_x = test_x.to_numpy()\n",
        "\n",
        "  return train_x ,train_y, test_x,test_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test code\n",
        "train_x ,train_y, test_x,test_y = preprocess_data(7,raw_data,make_feature='raw',norm=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89wvey9jlUPi",
        "outputId": "e3cf2850-9cf5-4174-9420-fdb8898d79af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spliting data 7 from raw_data.... \n",
            "training set this round includes [1, 2, 3, 4, 5, 6, 8, 9, 10]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtFkK2DIxYRt"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "oUPpDBuytYqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "def evaluate(model, test_x, test_y):\n",
        "    predictions = model.predict(test_x)\n",
        "    report = metrics.classification_report(test_y, predictions,output_dict=True,digits=3)\n",
        "    print('Model Performance')\n",
        "    print(report)\n",
        "    return report\n",
        "#---Preprocess-------\n",
        "data_path = r\"/content/drive/MyDrive/Data_chair/position_arakawa_lab_labeled.csv\"\n",
        "\n",
        "#read data in \n",
        "raw_data= pd.read_csv(data_path)\n",
        "recall =[]\n",
        "precision=[]\n",
        "accuracy=[]\n",
        "f1_score=[]\n",
        "#variable for ploting\n",
        "fig, axes = plt.subplots(4, 3,figsize=(50,50))\n",
        "plot_x=0\n",
        "plot_y=0\n",
        "\n",
        "# Validation approach:ã€€Leave one person out \n",
        "# Id i out \n",
        "for i in range(1,11):\n",
        "  # split_data(test_id, data, create feature method) methods can be 'raw''fifty' 'hundred'\n",
        "  train_x ,train_y, test_x,test_y = preprocess_data(i,raw_data,'fifty',norm=True)\n",
        "  # instantiate model\n",
        "  base_model = SVC(kernel='rbf', random_state = 42)\n",
        "  base_model.fit(train_x,train_y)\n",
        "  # evaluate\n",
        "  base_report = evaluate(base_model, test_x, test_y)\n",
        "  recall.append(base_report['macro avg']['recall'])\n",
        "  precision.append(base_report['macro avg']['precision'])\n",
        "  f1_score.append(base_report['macro avg']['f1-score'])\n",
        "  accuracy.append(base_report['accuracy'])\n",
        "\n",
        "  print(base_report['accuracy'])\n",
        "  print(base_report['macro avg'])\n",
        "\n",
        "  # testing\n",
        "  x_pred = base_model.predict(test_x)\n",
        "  x_pred = base_model.predict(test_x)\n",
        "  # display\n",
        "  ConfusionMatrixDisplay.from_predictions(test_y, x_pred,ax=axes[plot_y][plot_x] )\n",
        "  plot_x =plot_x+1\n",
        "  if(plot_x>2):\n",
        "      plot_x=0\n",
        "      plot_y=plot_y+1\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dR8c7VExwunD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(len(accuracy))\n",
        "acc_arr=np.array(accuracy)\n",
        "print(np.round(acc_arr,decimals=4))\n",
        "print(np.round(np.array(recall).mean(),decimals=4))\n",
        "print(np.round(np.array(precision).mean(),decimals=4))\n",
        "print(np.round(np.array(f1_score).mean(),decimals=4))\n",
        "print(np.round(acc_arr.mean(),decimals=4))\n"
      ],
      "metadata": {
        "id": "CVLvJLeF6vxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RF"
      ],
      "metadata": {
        "id": "AclfsqI3wSyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate(model, test_x, test_y):\n",
        "    predictions = model.predict(test_x)\n",
        "    report = metrics.classification_report(test_y, predictions,output_dict=True,digits=3)\n",
        "    print('Model Performance')\n",
        "    print(report)\n",
        "    return report\n",
        "#---Preprocess-------\n",
        "data_path = r\"/content/drive/MyDrive/Data_chair/position_arakawa_lab_labeled.csv\"\n",
        "\n",
        "#read data in \n",
        "raw_data= pd.read_csv(data_path)\n",
        "recall =[]\n",
        "precision=[]\n",
        "accuracy=[]\n",
        "f1_score=[]\n",
        "#variable for ploting\n",
        "fig, axes = plt.subplots(4, 3,figsize=(50,50))\n",
        "plot_x=0\n",
        "plot_y=0\n",
        "\n",
        "#Leave one person id i out \n",
        "for i in range(1,11):\n",
        "  \n",
        "  train_x ,train_y, test_x,test_y = preprocess_data(i,raw_data,'fifty')\n",
        "  base_model = RandomForestClassifier(max_depth= 24, n_estimators= 50, random_state = 42)\n",
        "  base_model.fit(train_x, train_y)\n",
        "  base_report = evaluate(base_model, test_x, test_y)\n",
        "  recall.append(base_report['macro avg']['recall'])\n",
        "  precision.append(base_report['macro avg']['precision'])\n",
        "  f1_score.append(base_report['macro avg']['f1-score'])\n",
        "  accuracy.append(base_report['accuracy'])\n",
        "  print(base_report['accuracy'])\n",
        "  print(base_report['macro avg'])\n",
        "\n",
        "  # Plot non-normalized confusion matrix\n",
        "\n",
        "  \n",
        "  x_pred = base_model.predict(test_x)\n",
        "  x_pred = base_model.predict(test_x)\n",
        "  ConfusionMatrixDisplay.from_predictions(test_y, x_pred,ax=axes[plot_y][plot_x] )\n",
        "  plot_x =plot_x+1\n",
        "  if(plot_x>2):\n",
        "      plot_x=0\n",
        "      plot_y=plot_y+1\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t2VD_j0Sm9S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(len(accuracy))\n",
        "acc_arr=np.array(accuracy)\n",
        "print(np.round(acc_arr,decimals=4))\n",
        "print(np.round(np.array(recall).mean(),decimals=4))\n",
        "print(np.round(np.array(precision).mean(),decimals=4))\n",
        "print(np.round(np.array(f1_score).mean(),decimals=4))\n",
        "print(np.round(acc_arr.mean(),decimals=4))"
      ],
      "metadata": {
        "id": "vii2dQEg6Phz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN"
      ],
      "metadata": {
        "id": "qcYiTWCV8SPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def evaluate(model, test_x, test_y):\n",
        "    predictions = model.predict(test_x)\n",
        "    report = metrics.classification_report(test_y, predictions,output_dict=True,digits=3)\n",
        "    print('Model Performance')\n",
        "    print(report)\n",
        "    return report\n",
        "#---Preprocess-------\n",
        "data_path = r\"/content/drive/MyDrive/Data_chair/position_arakawa_lab_labeled.csv\"\n",
        "\n",
        "#read data in \n",
        "raw_data= pd.read_csv(data_path)\n",
        "recall =[]\n",
        "precision=[]\n",
        "accuracy=[]\n",
        "f1_score=[]\n",
        "#variable for ploting\n",
        "fig, axes = plt.subplots(4, 3,figsize=(50,50))\n",
        "plot_x=0\n",
        "plot_y=0\n",
        "\n",
        "#Leave one person id i out \n",
        "for i in range(1,11):\n",
        "  \n",
        "  train_x ,train_y, test_x,test_y = preprocess_data(i,raw_data,'fifty',norm=True)\n",
        "  base_model = KNeighborsClassifier(n_neighbors=12)\n",
        "  base_model.fit(train_x,train_y)\n",
        "  base_report = evaluate(base_model, test_x, test_y)\n",
        "  recall.append(base_report['macro avg']['recall'])\n",
        "  precision.append(base_report['macro avg']['precision'])\n",
        "  f1_score.append(base_report['macro avg']['f1-score'])\n",
        "  accuracy.append(base_report['accuracy'])\n",
        "  print(base_report['accuracy'])\n",
        "  print(base_report['macro avg'])\n",
        "\n",
        "  # Plot non-normalized confusion matrix\n",
        "\n",
        "  \n",
        "  x_pred = base_model.predict(test_x)\n",
        "  x_pred = base_model.predict(test_x)\n",
        "  ConfusionMatrixDisplay.from_predictions(test_y, x_pred,ax=axes[plot_y][plot_x] )\n",
        "  plot_x =plot_x+1\n",
        "  if(plot_x>2):\n",
        "      plot_x=0\n",
        "      plot_y=plot_y+1\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pNeYzYuI8UK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(len(accuracy))\n",
        "acc_arr=np.array(accuracy)\n",
        "print(np.round(acc_arr,decimals=4))\n",
        "print(np.round(np.array(recall).mean(),decimals=4))\n",
        "print(np.round(np.array(precision).mean(),decimals=4))\n",
        "print(np.round(np.array(f1_score).mean(),decimals=4))\n",
        "print(np.round(acc_arr.mean(),decimals=4))\n"
      ],
      "metadata": {
        "id": "_o051BemxBt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN trial"
      ],
      "metadata": {
        "id": "jRkplTAF4bpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross Validation"
      ],
      "metadata": {
        "id": "VOlOseCEH4c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "\n",
        "class ChairDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.data = torch.from_numpy(X).float()\n",
        "        if y is not None:\n",
        "            y = y.astype(np.int)\n",
        "            self.label = torch.LongTensor(y)\n",
        "        else:\n",
        "            self.label = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is not None:\n",
        "            return self.data[idx], self.label[idx]\n",
        "        else:\n",
        "            return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "        \n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.layer1 = nn.Linear(72, 36)\n",
        "        self.dropout1= nn.Dropout(0.5)\n",
        "        self.layer2 = nn.Linear(36, 12)\n",
        "        self.act_fn = nn.ReLU()\n",
        "        self.out = nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x= self.dropout1(x)\n",
        "        x = self.act_fn(x)\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        x = self.act_fn(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x\n",
        "#----training function -----\t\t\n",
        "#check device\n",
        "def get_device():\n",
        "  return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# fix random seed\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed) #set generator's seed on cpu\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed) #set generator's seed on gpu\n",
        "        torch.cuda.manual_seed_all(seed)  #set generator's seed on all gpu device\n",
        "    np.random.seed(seed)  \n",
        "    torch.backends.cudnn.benchmark = False # if this is set to true, torh may use different convolution algorithm backend which has faster speed=> irreproducible result.\n",
        "    torch.backends.cudnn.deterministic = True #only use deterministic convolution algorithms.\n",
        "#----end training-----\n",
        "\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "\n",
        "\tdata_path = r\"/content/drive/MyDrive/Data_chair/position_arakawa_lab_labeled.csv\"\n",
        "\n",
        "\t#read in data \n",
        "\traw_data= pd.read_csv(data_path)\n",
        "\n",
        "\t#variable for ploting\n",
        "\tfig, axes = plt.subplots(4, 3,figsize=(50,50))\n",
        "\tplot_x=0\n",
        "\tplot_y=0\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  #---Cross Validate------\n",
        "\tacc_list=[]\n",
        "\treport_list = []\n",
        "\tfor cross_id in range(1,11):\n",
        "\t\tBATCH_SIZE = 10\n",
        "\n",
        "\t\ttrain_x ,train_y, test_x,test_y = preprocess_data(cross_id,raw_data,make_feature='fifty',norm=True)\n",
        "\t\t# adjust value to suit cross entorpy\n",
        "\t\ttrain_y= train_y-1\n",
        "\t\ttest_y= test_y-1\n",
        "\n",
        "\t\ttrain_set = ChairDataset(train_x, train_y)\n",
        "\t\ttest_set = ChairDataset(test_x, test_y)\n",
        "\t\ttrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False ) #only shuffle the training data\n",
        "\t\ttest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\t\n",
        "\t\t#----training setting-----\n",
        "\t\t# fix random seed for reproducibility\n",
        "\t\tsame_seeds(0)\n",
        "\n",
        "\t\t# get device \n",
        "\t\tdevice = get_device()\n",
        "\t\t#device = 'cpu'\n",
        "\t\tprint(f'DEVICE: {device}')\n",
        "\n",
        "\t\t# training parameters\n",
        "\t\tnum_epoch = 110          # number of training epoch\n",
        "\t\tlearning_rate = 0.0001       # learning rate\n",
        "\n",
        "\t\t# the path where checkpoint saved\n",
        "\t\tmodel_path = './model.ckpt'\n",
        "\n",
        "\t\t# create model, define a loss function, and optimizer\n",
        "\t\tmodel = Classifier().to(device)\n",
        "\t\tcriterion = nn.CrossEntropyLoss() \n",
        "\t\toptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay=0.0001)\n",
        "\n",
        "\t\t#----start training -----\n",
        "\t\t# start training\n",
        "\n",
        "\t\tbest_acc = 0.0\n",
        "\t\tfor epoch in range(num_epoch):\n",
        "\t\t\ttrain_acc = 0.0\n",
        "\t\t\ttrain_loss = 0.0\n",
        "\t\t\tval_acc = 0.0\n",
        "\t\t\tval_loss = 0.0\n",
        "\n",
        "\t\t\t# training\n",
        "\t\t\tmodel.train() # set the model to training mode\n",
        "\t\t\tfor i, data in enumerate(train_loader):\n",
        "\t\t\t\tinputs, labels = data\n",
        "\t\t\t\tinputs, labels = inputs.to(device), labels.to(device)\n",
        "\t\t\t\toptimizer.zero_grad() \n",
        "\t\t\t\toutputs = model(inputs) \n",
        "\t\t\t\tbatch_loss = criterion(outputs, labels)\n",
        "\t\t\t\t_, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "\t\t\t\tbatch_loss.backward() \n",
        "\t\t\t\toptimizer.step() \n",
        "\n",
        "\t\t\t\ttrain_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
        "\t\t\t\ttrain_loss += batch_loss.item()\n",
        "\n",
        "\t\t\t# validation\n",
        "\t\t\tif len(test_set) > 0:\n",
        "\t\t\t\tmodel.eval() # set the model to evaluation mode\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfor i, data in enumerate(test_loader):\n",
        "\t\t\t\t\t\tinputs, labels = data\n",
        "\t\t\t\t\t\tinputs, labels = inputs.to(device), labels.to(device)\n",
        "\t\t\t\t\t\toutputs = model(inputs)\n",
        "\t\t\t\t\t\tbatch_loss = criterion(outputs, labels) \n",
        "\t\t\t\t\t\t_, val_pred = torch.max(outputs, 1) \n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\t\tval_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
        "\t\t\t\t\t\tval_loss += batch_loss.item()\n",
        "\n",
        "\t\t\t\t\tprint('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
        "\t\t\t\t\t\tepoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(test_set), val_loss/len(test_loader)\n",
        "\t\t\t\t\t))\n",
        "\n",
        "\t\t\t\t\t# if the model improves, save a checkpoint at this epoch\n",
        "\t\t\t\t\tif val_acc > best_acc:\n",
        "\t\t\t\t\t\tbest_acc = val_acc\n",
        "\t\t\t\t\t\ttorch.save(model.state_dict(), model_path)\n",
        "\t\t\t\t\t\tprint('saving model with acc {:.3f}'.format(best_acc/len(test_set)))\n",
        "\t\t\telse:\n",
        "\t\t\t\tprint('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
        "\t\t\t\t\tepoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
        "\t\t\t\t))\n",
        "\n",
        "\t\t# if not validating, save the last epoch\n",
        "\t\tif len(test_set) == 0:\n",
        "\t\t\tacc_list.append(best_acc/len(test_set))\n",
        "\t\t\ttorch.save(model.state_dict(), model_path)\n",
        "\t\t\tprint('saving model at last epoch')\n",
        "\t\t\n",
        "\t\t#---Evaluate-----\n",
        "\t\t# create model and load weights from checkpoint\n",
        "\t\tmodel = Classifier().to(device)\n",
        "\t\tmodel.load_state_dict(torch.load(model_path))\n",
        "\t\tpredict = []\n",
        "\t\tmodel.eval() # set the model to evaluation mode\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tfor i, data in enumerate(test_loader):\n",
        "\t\t\t\tinputs, labels= data\n",
        "\t\t\t\tinputs = inputs.to(device)\n",
        "\t\t\t\toutputs = model(inputs)\n",
        "\t\t\t\t_, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "\n",
        "\t\t\t\tfor y in test_pred.cpu().numpy():\n",
        "\t\t\t\t\tpredict.append(y)\n",
        "\t\tcorrect = (torch.tensor(predict) ==torch.tensor(test_y)).sum()\n",
        "\t\tacc = correct/test_x.shape[0]\t\n",
        "\t\tacc_list.append(round(float(acc),2))\n",
        "\t\treport_list.append(precision_recall_fscore_support(torch.tensor(test_y),torch.tensor(predict),average='macro'))\n",
        "\t\tConfusionMatrixDisplay.from_predictions(test_y, predict,ax=axes[plot_y][plot_x] )\n",
        "  \n",
        "\t\tplot_x =plot_x+1\n",
        "\t\tif(plot_x>2):\n",
        "\t\t\tplot_x=0\n",
        "\t\t\tplot_y=plot_y+1\n",
        "\t\t\n",
        "\t\t\n",
        "\t#---- end for loop----\n",
        "\tplt.show()\t\n",
        "\t\n",
        "\tprint(acc_list)\n",
        "\t"
      ],
      "metadata": {
        "id": "ReDAkvKsU34K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_arr= np.array(acc_list).mean(axis=0)\n",
        "report_arr= np.array(report_list)[:,:-1].mean(axis=0)\n",
        "print(np.round(acc_arr.astype(float),decimals=4))\n",
        "print(np.round(report_arr.astype(float),decimals=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ztj1MRU8IBJc",
        "outputId": "94cea3d2-a441-4026-db58-241e45a61903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.851\n",
            "[0.8146 0.85   0.8143]\n"
          ]
        }
      ]
    }
  ]
}